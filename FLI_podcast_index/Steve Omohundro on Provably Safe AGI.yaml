title: Steve Omohundro on Provably Safe AGI
original_url: '"https://zencastr.com/z/r9oXs86b"'
date_published: '2023-10-05'
host:
  name: Gus Stocker
  affiliation: Future of Life Institute
guests:
- name: Steve Omohundro
  affiliation: ''
  title: AI Researcher
  bio: Steve Omohundro is a physicist and AI researcher known for his work on AI safety,
    basic AI drives, and provably safe systems.
  expertise_areas:
  - Artificial Intelligence
  - AI Safety
  - Mathematical Proofs
synopsis: In this episode, Steve Omohundro discusses the urgent need for provably
  safe AI systems, emphasizing the existential risks posed by AGI and the importance
  of implementing robust safety measures. He shares insights on self-improving AI,
  fundamental drives, and the collaboration with Max Tegmark on their recent paper.
key_topics:
- AI Safety
- Provably Safe AGI
- Self-Improving AI
- Basic AI Drives
- Mathematical Proofs in AI
tags:
- AI
- Safety
- Technology
- Research
key_points:
- AGI brings existential risks that require immediate attention.
- Traditional alignment approaches may not address safety concerns.
- The need for guardrails around powerful AI systems to prevent misuse.
- Mathematical proofs can ensure system safety and reliability.
- Self-improving AI poses significant challenges and risks.
segments:
- title: Introduction and Guest Welcome
  summary: Gus Stocker welcomes Steve Omohundro to discuss his expertise in AI and
    recent work related to AI safety.
  key_quotes:
  - quote: Thank you for having me on the podcast.
    speaker: Steve Omohundro
    context: Initial welcome statement.
- title: AI Safety and Early Interests
  summary: Steve reflects on his journey in AI, early realizations about AI risks,
    and his shift towards focusing on AI safety.
  key_quotes:
  - quote: Early on, I thought AI was an unabashed good.
    speaker: Steve Omohundro
    context: Discussing his initial optimism about AI.
- title: Realization of AI Risks and Dual Focus
  summary: Discussion evolves to the potential dangers associated with AI and the
    need to balance capabilities with risk mitigation.
  key_quotes:
  - quote: '"Since then, I''ve been doing both the capabilities and the risk side
      of things."'
    speaker: Steve Omohundro
    context: Transitioning into AI safety discussions.
- title: The Concept of Self-Improving AI
  summary: Steve explains the concept of self-improving AI and the implications of
    allowing AI systems to enhance themselves autonomously.
  key_quotes:
  - quote: '"When you have a self-improving system and you keep improving yourself,
      where does it go?"'
    speaker: Steve Omohundro
    context: Exploring the potential pathways of self-improvement in AI.
- title: Provably Safe Systems
  summary: '"The discussion pivots to Steve''s paper on provably safe AI, outlining
    their approach and necessity for guardrails."'
  key_quotes:
  - quote: What we really need are guardrails around these risky things.
    speaker: Steve Omohundro
    context: Addressing the need for protective measures for AI systems.
- title: Security Measures and Bioterrorism Risks
  summary: Steve discusses security measures needed to prevent AGIs from misusing
    technology, backing it with examples of potential threats.
  key_quotes:
  - quote: '"We need to ensure that they''re only doing something which is actually
      safe."'
    speaker: Steve Omohundro
    context: Emphasizing the critical need for safety in AI operations.
- title: Actionable Insights and Future Directions
  summary: Steve and Gus summarize key insights, calling for proactive measures in
    AI development and implementation.
  key_quotes:
  - quote: If we want to ensure safety, we need guarantees driven by mathematical
      proof.
    speaker: Steve Omohundro
    context: Conclusion emphasizing the importance of a proof-based approach.
websites_referenced:
- url: '"https://centerforaisafety.org"'
  name: Center for AI Safety
  context: Organization discussing many risks associated with AI.
  access_date: '2023-10-05'
tools_mentioned:
- name: AlphaFold
  url: ''
  description: AI system for predicting the 3D structure of proteins.
  context: Example of AI technologies with potential risks.
research_papers:
- title: Provably Safe Systems
  authors:
  - Max Tegmark
  - Steve Omohundro
  year: 2023
  url: ''
  key_findings: Advocates for the development of mechanisms to ensure safety in AGI
    to prevent misuse.
  context: The paper discussed throughout the episode.
books_referenced:
- title: ''
  authors: []
  year: null
  isbn: ''
  context: ''
organizations:
  academic_institutions:
  - name: University of California, Berkeley
    location: Berkeley, CA
    context: '"Steve Omohundro''s alma mater where he conducted his PhD."'
  companies:
  - name: OpenAI
    industry: Artificial Intelligence
    context: Leading organization in AI with ongoing safety initiatives.
  - name: Nvidia
    industry: Technology
    context: Company producing powerful GPUs relevant for AI training.
  non_profits:
  - name: Future of Life Institute
    focus_area: AI safety and existential risks.
    context: Organization hosting the podcast.
  government_agencies:
  - name: ''
    country: ''
    context: ''
notable_people:
  academics:
  - name: Eliezer Yudkowsky
    institution: Machine Intelligence Research Institute
    field: AI Safety
    context: Influential figure in the AI safety community, mentioned for his warnings.
  industry_leaders:
  - name: Sam Altman
    company: OpenAI
    role: CEO
    context: Notable influencer in AI and policy discussions.
projects_mentioned:
- name: Foresight Institute
  organization: Foresight Institute
  description: Focus on nanotechnology and associated risks.
  status: Active
  url: ''
  context: ''
events:
  historical:
  - name: Deep Learning Revolution
    date: '2012'
    significance: Marked a major breakthrough in AI capabilities.
    context: Transition from traditional AI to deep learning approaches.
  upcoming:
  - name: ''
    date: null
    location: ''
    description: ''
    url: ''
keywords:
- AGI
- AI Risk
- Safety Protocols
- Mathematical Proof
categories:
- Technology
- Artificial Intelligence
- Research
intended_audience: Researchers, policymakers, AI enthusiasts.
expertise_level: Intermediate to expert
recommended_reading:
- title: '"The Alignment Problem: Machine Learning and Human Values"'
  url: ''
  type: book
  relevance: Insights on aligning AI technologies with human ethics.
action_items:
- Support research initiatives focused on AI safety.
- Advocate for the development of provably safe AI systems.
controversial_topics:
- topic: AI Alignment Approaches
  different_viewpoints:
  - perspective: Focus on aligning AI with human values.
    proponents: Mainstream AI researchers.
    key_arguments: Alignment is essential for safe AI deployment.
  - perspective: Guardrails and provability.
    proponents: Steve Omohundro and Max Tegmark.
    key_arguments: Alignment approaches may not adequately prevent risks.
