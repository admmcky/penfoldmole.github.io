title: Max Tegmark and the FLI Team on 2020 and Existential Risk Reduction in the
  New Year
original_url: '"https://zencastr.com/z/K8CFRejA"'
date_published: 2021-01-08
host:
  name: Lucas Perry
  affiliation: Future of Life Institute
guests:
- name: Max Tegmark
  affiliation: MIT
  title: Professor, President at Future of Life Institute
  bio: Max Tegmark is a professor at MIT and president of the Future of Life Institute,
    focusing on machine learning and physics research.
  expertise_areas:
  - Machine learning
  - Physics
- name: David Nicholson
  affiliation: Future of Life Institute
  title: Team Member
  bio: David Nicholson is a member of the Future of Life Institute team.
  expertise_areas:
  - Global health
  - Existential risks
- name: Emilia Joworski
  affiliation: Future of Life Institute
  title: Team Member
  bio: Emilia Joworski works at the Future of Life Institute and has engaged in initiatives
    on digital cooperation.
  expertise_areas:
  - AI ethics
  - Digital cooperation
- name: Tucker Davy
  affiliation: Future of Life Institute
  title: Team Member
  bio: Tucker Davy is a team member at the Future of Life Institute working on various
    projects.
  expertise_areas:
  - AI safety
  - Public outreach
synopsis: '"The episode reviews the Future of Life Institute''s work in 2020 and discusses
  key learnings for 2021, including reflections on existential risks and the importance
  of international cooperation."'
key_topics:
- Existential risk
- International cooperation
- AI ethics
- Public health
tags:
- Existential Risk
- Artificial Intelligence
- Public Health
- International Cooperation
key_points:
- International cooperation is crucial for addressing global health threats.
- Trust in public health systems is necessary for effective pandemic responses.
- AI systems should prioritize user interests over corporate interests.
- Governance of AI requires robust engagement and understanding from policymakers.
segments:
- title: Introduction to FLI Podcast 2020 Review
  summary: '"Lucas Perry introduces the podcast and outlines its focus on FLI''s achievements
    in 2020, specifically centered on existential risks."'
  key_quotes:
  - quote: '"Let''s start off the new year by jumping into our first question."'
    speaker: Lucas Perry
    context: Introduction to the episode.
- title: Celebrating Unsung Heroes in Risk Reduction
  summary: '"Max Tegmark shares insights on the Future of Life Award, recognizing
    individuals who contributed significantly to humanity''s safety."'
  key_quotes:
  - quote: The first award went to Vasily Arkupov, who single-handedly prevented a
      Soviet nuclear attack on the US Navy.
    speaker: Max Tegmark
    context: Discussion on the Future of Life Award.
- title: AI Loyalty and User Interests
  summary: The discussion shifts to the ethical considerations of AI systems and the
    importance of ensuring they prioritize user interests.
  key_quotes:
  - quote: '"Should they ultimately be working for some giant tech firm and our interest
      is only incidental?"'
    speaker: Max Tegmark
    context: Exploration of conflicts of interest in AI.
- title: Lessons from Smallpox Eradication
  summary: David Nicholson discusses the importance of international cooperation and
    social trust shown during the smallpox eradication efforts.
  key_quotes:
  - quote: International cooperation between states is crucial to address global challenges.
    speaker: David Nicholson
    context: Reflections on historical success in combating diseases.
- title: Promoting Human Rights in AI
  summary: '"Emilia Joworski outlines FLI''s role in advocating for human rights-based
    AI recommendations within global initiatives."'
  key_quotes:
  - quote: Life and death decisions must not be delegated to machines.
    speaker: Emilia Joworski
    context: Discussing the ethical framework for AI.
- title: Reflection on Global Vulnerability in 2020
  summary: Contributors reflect on the lessons learned from 2020, particularly concerning
    the COVID-19 pandemic and its implications on existential risks.
  key_quotes:
  - quote: Humanity is more fragile than many would have liked to think.
    speaker: Max Tegmark
    context: Reflecting on global vulnerability revealed by the pandemic.
websites_referenced:
- url: '"https://futureoflife.org"'
  name: Future of Life Institute
  context: Website for the Future of Life Institute, detailing their projects and
    initiatives.
  access_date: null
tools_mentioned:
- name: Pindex
  url: '"https://www.pindex.org/"'
  description: Video platform for educational videos.
  context: Referenced in context with a video about AI and existential risks.
research_papers:
- title: '"AI Loyalty: A New Paradigm for Aligning Stakeholder Interests"'
  authors:
  - Max Tegmark
  - Guy Dempsey
  - Harry Surdin
  - Peter Reiner
  year: 2020
  url: '"https://futureoflife.org/ai-loyalty/"'
  key_findings: Proposes standards for AI systems designed to prioritize user interests.
  context: Discussion around AI ethics and responsibilities.
books_referenced:
- title: The Biography of Viktor Zhdanov
  authors:
  - Elena Zhdanova
  year: null
  isbn: ''
  context: '"Biography detailing Zhdanov''s contributions to global health."'
organizations:
  academic_institutions:
  - name: MIT
    location: Cambridge, MA
    context: Home to Max Tegmark and a prominent institution in science and technology.
  companies:
  - name: Amazon
    industry: E-commerce
    context: Example cited regarding AI loyalty and corporate interests.
  non_profits:
  - name: Future of Life Institute
    focus_area: Existential risk reduction and AI safety
    context: The organization hosting the podcast.
  government_agencies:
  - name: World Health Organization
    country: Global
    context: Key agency referenced in the smallpox eradication discussion.
notable_people:
  academics:
  - name: Max Tegmark
    institution: MIT
    field: Physics, Machine Learning
    context: President of the Future of Life Institute.
  industry_leaders:
  - name: Viktor Zhdanov
    company: N/A
    role: Virologist
    context: Awarded for his work in smallpox eradication.
  policy_makers:
  - name: Bill Foege
    role: Epidemiologist
    organization: Centers for Disease Control and Prevention (CDC)
    context: Key figure in smallpox eradication.
projects_mentioned:
- name: Future of Life Award
  organization: Future of Life Institute
  description: An award recognizing individuals for their contributions to risk reduction.
  status: Ongoing
  url: '"https://futureoflife.org/future-of-life-award/"'
  context: Recognizes contributions that enhance human safety and global health.
events:
  historical:
  - name: Smallpox Eradication
    date: 1980
    significance: The World Health Organization declared smallpox eradicated, a significant
      public health achievement.
    context: Reference to the collaborative efforts of global health organizations
      during the Cold War.
  upcoming:
  - name: Future of Life Institute Initiatives
    date: null
    location: Global
    description: Various initiatives planned for 2021 to focus on existential risk
      reduction.
    url: '"https://futureoflife.org/"'
keywords:
- digital cooperation
- existential risk
- health security
- AI safety
categories:
- Technology
- Public Health
- Ethics
- Global Policy
intended_audience: Individuals interested in global risks, technology, and ethics.
expertise_level: Intermediate
recommended_reading:
- title: '"Living in a Fragile World: Lessons from 2020"'
  url: ''
  type: article
  relevance: Reflection on how COVID-19 highlights the importance of understanding
    global risks.
action_items:
- Encourage awareness and engagement on existential risks among policymakers and the
  public.
- Promote international cooperation to address global health threats.
controversial_topics:
- topic: AI Ethics and Corporate Influence
  different_viewpoints:
  - perspective: AI should prioritize user needs over corporate profit.
    proponents: Max Tegmark, David Nicholson
    key_arguments: AI systems currently favor corporate interests, potentially undermining
      user trust.
  - perspective: Corporations are necessary for funding and developing AI technologies.
    proponents: Industry leaders
    key_arguments: Profit incentives drive innovation, which can also benefit users.
